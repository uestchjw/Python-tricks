
optimizer 和 scheduler 的区别：

  optimizer:其作用在于根据选用不同的优化器以及设置的lr、momentum等(超参数)对网络模型的参数进行更新，更新的方法是optimizer.step(),一般在每个batch后更新
  # 优化方式为mini-batch momentum-SGD，并采用L2正则化（权重衰减） momentum：动量 weight_decay=1e-2
    optimizer = optim.SGD(model.parameters(), lr=config.LR,momentum=0.9, weight_decay=config.weight_decay)
    
  scheduler:其作用在于对optimizer中的学习率进行更新、调整，更新的方法是scheduler.step()，一般在每个epoch后更新
    例如：等间隔调整学习率 每过step_size个epoch,lr = lr*gamma   100轮训练，李寅龙将step_size设为12，初始学习率为0.005
    scheduler = lr_scheduler.StepLR(optimizer, step_size=config.step_size, gamma=0.5) 






权重衰减（也叫L2正则化)的作用？
  1.令w更小，根据奥卡姆剃刀原理，表示网络的复杂度更低，对数据的拟合更好，并且实际证明了L2正则化效果往往好于不正则化
  2.从数学角度，过拟合时，拟合函数的系数往往非常大，因为要考虑每个点，所以要变化非常快，所以导数要很大，所以系数很大









SGD 和 Adam 优化器的区别：








criterion = nn.CrossEntropyLoss() 交叉熵损失函数的计算方法？
# criterion = LabelSmoothingCrossEntropy() 标签平滑，减小过拟合

  交叉熵：比如 有3类：模型给出的概率分别是0.1 0.2 0.7; 真实的情况是ont hot 0 0 1; 那么损失函数L=-(0*log0.1+0*log0.2+1*log0.7)
  labelsmoothing标签平滑：把one hot码进行了改变，增加了一个小量 如 0.05 0.05 0.9 
    1.因为有的标注分类可能并不正确






softmax函数的计算方式？
  softmax函数的作用就是将输出的各种各样的结果，转化为概率
  我们都知道，概率值有2个基本条件：非负数、求和为1
  所以：softmax计算第i类的概率
    第一步：exp(input_i)
    第二步：P(i) = exp(input_i)/exp求和
  
  对于二分类问题，最后的函数采用sigmold还是softmax是有区别的
  1个值给到sigmold,2个值给到softmax
  对于sigmold: output(x1) = 1/(1+exp(-x1))
  对于softmax: output(x1) = exp(x1)/(exp(x1)+exp(x2))
  对于CV，sigmold稍好一点；对于NLP，softmax稍好一点    某个博主的经验
  
